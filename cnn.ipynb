{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore (train_set) AKA (train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_set)             #trainset[IMAGEINDEX][0-image_tensor,1-class] [CHANNEL][HEIGHT][WIDTH]\n",
    "print(train_set.data.shape)  #trainset.data[IMAGE INDEX][HEIGHT][WIDTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# SINGLE\n",
    "image , label = next(iter(train_set))   # SAME AS train_set[0] - train_set IS INDEXABLE\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# BATCH\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(train_loader)) #CANT USE train_loader[0] - NOT INDEXABLE\n",
    "images , labels = batch          #LOADS 1 BATCH(10 images) AT ONCE\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = F.cross_entropy\n",
    "        \n",
    "    def forward(self, t):\n",
    "        #conv layer 1\n",
    "        t = F.relu(self.conv1(t))        \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #conv layer 2\n",
    "        t = F.relu(self.conv2(t)) \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        t = F.relu(self.fc1(t.reshape(-1, 12*4*4)))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)   #NO NEED BECAUSE LOSSFUNCTION AUTOMATICALL DOES CROSS ENTROPY\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=20)\n",
    "network = Network(lr = 0.01)\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">wandb-run-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nimishsantosh/pytorch-course\" target=\"_blank\">https://wandb.ai/nimishsantosh/pytorch-course</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nimishsantosh/pytorch-course/runs/j87xz34a\" target=\"_blank\">https://wandb.ai/nimishsantosh/pytorch-course/runs/j87xz34a</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201028_153905-j87xz34a</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset\": \"FashionMNIST\",\n",
    "    \"model\": \"Custom CNN\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 20,\n",
    "    \"num_epochs\": 15\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"pytorch-course\",\n",
    "                 name=\"wandb-run-2\",\n",
    "                 tags=[\"pytorch\",\"cnn\",\"fashion-mnist\"],\n",
    "                 config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0    LOSS:  1691.441497899592\n",
      "EPOCH:  1    LOSS:  1386.9597552157938\n",
      "EPOCH:  2    LOSS:  1336.5541762076318\n",
      "EPOCH:  3    LOSS:  1258.9299256540835\n",
      "EPOCH:  4    LOSS:  1291.1713772583753\n",
      "EPOCH:  5    LOSS:  1294.235687110573\n",
      "EPOCH:  6    LOSS:  1305.2817852273583\n",
      "EPOCH:  7    LOSS:  1263.895076436922\n",
      "EPOCH:  8    LOSS:  1354.8627365492284\n",
      "EPOCH:  9    LOSS:  1224.0199121646583\n",
      "EPOCH:  10    LOSS:  1334.1979698315263\n",
      "EPOCH:  11    LOSS:  1238.2312612347305\n",
      "EPOCH:  12    LOSS:  1299.0315858386457\n",
      "EPOCH:  13    LOSS:  1312.0764766652137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32197<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa348d0e93794166a1f251a84b259bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.13MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.00582168873…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201028_153905-j87xz34a/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201028_153905-j87xz34a/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>EPOCH</td><td>13</td></tr><tr><td>LOSS</td><td>1312.07648</td></tr><tr><td>_step</td><td>13</td></tr><tr><td>_runtime</td><td>500</td></tr><tr><td>_timestamp</td><td>1603880248</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>EPOCH</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>LOSS</td><td>█▃▃▂▂▂▂▂▃▁▃▁▂▂</td></tr><tr><td>_step</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>_runtime</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">wandb-run-2</strong>: <a href=\"https://wandb.ai/nimishsantosh/pytorch-course/runs/j87xz34a\" target=\"_blank\">https://wandb.ai/nimishsantosh/pytorch-course/runs/j87xz34a</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        preds = network(images)\n",
    "        loss = network.loss(preds, labels) #calc loss\n",
    "        \n",
    "        network.optimizer.zero_grad()      #set gradients to zero\n",
    "        loss.backward()                    #calc gradients\n",
    "        network.optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(\"EPOCH: \",epoch,\"   LOSS: \",total_loss)\n",
    "    wandb.log({'EPOCH': epoch, 'LOSS': total_loss})\n",
    "    \n",
    "torch.save(network.state_dict(), './models/model1.pt')\n",
    "wandb.save('./models/model1.pt')\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix / Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(train_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        preds = model(images)\n",
    "        \n",
    "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "    return all_preds\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():      #DONT TRACK COMPUTATIONS BECAUSE WE ARENT TRAINING\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "    train_preds = get_all_preds(network, prediction_loader)\n",
    "\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50862\n"
     ]
    }
   ],
   "source": [
    "num_correct = get_num_correct(train_preds, train_set.targets)\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.8477\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY: \",num_correct / len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 0, 3,  ..., 3, 0, 5])\n",
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "## FOR CONFUSION MATRIX\n",
    "matrix = torch.zeros(10,10,dtype = torch.int32)\n",
    "\n",
    "preds_argmax = train_preds.argmax(dim=1)\n",
    "print(preds_argmax)\n",
    "pred_labels = train_set.targets\n",
    "print(pred_labels)\n",
    "\n",
    "for i in range(len(preds_argmax)):\n",
    "    matrix[pred_labels[i],preds_argmax[i]] += 1 #create matrix for every predictioin,label pair increment corresponding box in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5179,    5,   60,  293,   32,    2,  401,    0,   28,    0],\n",
       "        [   4, 5664,    2,  279,    8,    1,   36,    0,    6,    0],\n",
       "        [  43,    5, 4694,   30,  733,    2,  478,    0,   14,    1],\n",
       "        [ 119,   20,   12, 5328,  348,    0,  172,    0,    1,    0],\n",
       "        [   6,    4,  894,  171, 4522,    0,  395,    0,    8,    0],\n",
       "        [   8,    0,    1,    0,    0, 5561,  115,  255,    8,   52],\n",
       "        [1544,    7,  883,  169,  597,    2, 2755,    0,   43,    0],\n",
       "        [   0,    0,    0,    0,    0,   64,   33, 5844,    7,   52],\n",
       "        [   7,    1,   21,   14,   43,    8,  126,    3, 5777,    0],\n",
       "        [   2,    0,    0,    0,    0,   53,   23,  378,    6, 5538]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1324e4a90>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
